import os
import av
import json
import torch
import glob
import importlib
import numpy as np
import math


def derangement(lst):
    while True:
        shuffled = lst[:]
        random.shuffle(shuffled)
        if all(original != shuffled[i] for i, original in enumerate(lst)):
            return shuffled


def normalize(x):
    return x / x.norm(dim=-1, keepdim=True)






def pad_tensor_to_length(tensor, target_length, dim=1):
    # Calculate padding: (0, 0) means no padding for the last dimension
    # (padding_left, padding_right) for the sequence dimension
    current_length = tensor.size(dim)
    padding_left = 0
    padding_right = target_length - current_length
    
    # Create a padding configuration: (padding_left, padding_right, 0, 0) for each dimension if needed
    pad_config = (0, 0) * (dim) + (padding_left, padding_right) + (0, 0) * (tensor.dim() - dim - 1)
    
    # Apply padding
    padded_tensor = F.pad(tensor, pad_config)
    return padded_tensor


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def instantiate_from_config(config):
    """
    Instantiates an object based on a configuration.

    Args:
        config (dict): Configuration dictionary with 'target' and 'params'.

    Returns:
        object: An instantiated object based on the configuration.
    """
    if 'target' not in config:
        raise KeyError('Expected key "target" to instantiate.')
    return get_obj_from_str(config["target"])(**config.get("params", dict()))


def get_obj_from_str(string, reload=False):
    """
    Get an object from a string reference.

    Args:
        string (str): The string reference to the object.
        reload (bool): If True, reload the module before getting the object.

    Returns:
        object: The object referenced by the string.
    """
    module, cls = string.rsplit('.', 1)
    if reload:
        module_imp = importlib.import_module(module)
        importlib.reload(module_imp)
    return getattr(importlib.import_module(module, package=None), cls)


def create_mask(seq_lengths: list, device="cpu"):
    """
    Creates a mask tensor based on sequence lengths.

    Args:
        seq_lengths (list): A list of sequence lengths.
        device (str): The device to create the mask on.

    Returns:
        torch.Tensor: A mask tensor.
    """
    max_len = max(seq_lengths)
    mask = torch.arange(max_len, device=device)[None, :] < torch.tensor(seq_lengths, device=device)[:, None]
    return mask.to(torch.bool)




        



def sliding_window(tensor, window_size, overlap_size, temp_dim=0):
    """
    Apply a sliding window to a tensor.

    Args:
        tensor (torch.Tensor): The input tensor.
        window_size (int): The size of the window.
        overlap_size (int): The overlap size between windows.

    Returns:
        torch.Tensor: Tensor after applying the sliding window.
    """
    step_size = window_size - overlap_size # stride
    windows = tensor.unfold(temp_dim, window_size, step_size)
    return windows


def sliding_window_for_list(data_list, window_size, overlap_size):
    """
    Apply a sliding window to a list.

    Args:
        data_list (list): The input list.
        window_size (int): The size of the window.
        overlap_size (int): The overlap size between windows.

    Returns:
        list of lists: List after applying the sliding window.
    """
    step_size = window_size - overlap_size
    windows = [data_list[i:i + window_size] for i in range(0, len(data_list), step_size) if i + window_size <= len(data_list)]
    return windows


def sample_frame_indices(clip_len, frame_sample_rate, seg_len):
    '''
    Sample a given number of frame indices from the video. Extend by repeating the last index if necessary.
    Args:
        clip_len (int): Total number of frames to sample.
        frame_sample_rate (int): Sample every n-th frame.
        seg_len (int): Maximum allowed index of sample's last frame.
    Returns:
        indices (List[int]): List of sampled frame indices
    '''
    # Calculate the length of video needed after applying frame sample rate
    converted_len = int(clip_len * frame_sample_rate)

    # Ensure converted_len does not exceed the segment length
    converted_len = min(converted_len, seg_len)

    # Calculate start and end indices
    end_idx = min(seg_len, converted_len)
    start_idx = max(0, end_idx - converted_len)

    # Generate the indices from start to end
    if converted_len > 0:
        indices = np.linspace(start_idx, end_idx, num=min(clip_len, end_idx - start_idx), endpoint=False)
    else:
        indices = np.array([])

    # If the segment is too short, extend by repeating the last index
    if len(indices) < clip_len:
        last_index = indices[-1] if indices.size > 0 else 0  # Use the last index or 0 if no indices
        extra_indices = np.full(clip_len - len(indices), last_index)
        indices = np.concatenate([indices, extra_indices])

    indices = np.clip(indices, 0, seg_len - 1).astype(int)
    return indices.tolist()


def generate_tube_mask(input_size=(8, 14, 14), mask_ratio=0.90, verbose=False):
    """
    Generates a mask for video frames, applying the mask to each frame.
    
    Args:
        input_size (tuple): A tuple containing the number of frames, height, and width (frames, height, width).
        mask_ratio (float): The fraction of the grid to be masked (e.g., 0.9 for 90% masking).

    Returns:
        np.array: A flattened array representing the mask applied across all frames.
    """
    # Unpack input size
    frames, height, width = input_size
    num_patches_per_frame = height * width
    total_patches = frames * num_patches_per_frame
    num_masks_per_frame = int(mask_ratio * num_patches_per_frame)
    total_masks = frames * num_masks_per_frame

    # Generate mask for a single frame
    mask_per_frame = np.hstack([
        np.zeros(num_patches_per_frame - num_masks_per_frame, dtype=int),
        np.ones(num_masks_per_frame, dtype=int)
    ])
    np.random.shuffle(mask_per_frame)

    # Repeat the mask for each frame and flatten it to create a continuous mask array
    mask = np.tile(mask_per_frame, (frames, 1)).flatten()

    # Optionally, print or log mask information
    if verbose:
        print(f"Total patches: {total_patches}, Masked patches: {total_masks}")

    return mask


